library(sf)
library(data.table)
library(plyr)
library(ggplot2)
library(RColorBrewer)
library(gridExtra)
library(ggrepel)
library(stringr)
library(extrafont)
library(hrbrthemes)
library(VennDiagram)
library(ggtern)
library(kableExtra)
library(compositions)
library(lme4)
library(nlme)
library(gamlss)
library(gamlss.dist)
library(gamboostLSS)
library(ggeffects)
library(feather)
library(rprojroot)

rootdir <- find_root(has_dir("src"))
<<<<<<< HEAD
=======
# rootdir<-"C:/Users/ailene.ettinger/Box/floodrisk/flood_vulnerability/flood_vulnerability"
>>>>>>> 740dd296cfa453ad08aa61dca1f8f1491c3b0396
datadir = file.path(rootdir, 'data')
resdir = file.path(rootdir, 'results')
figdir = file.path(resdir, 'figures')
gdb <- 'flood_risk.gdb'
setwd(resdir)

#+----------------------------------- Define functions ------------------------------------------------
#Compute percentages for each race for a given model
summod <- function(dt, mod, ref, byc=NULL) {
  races <- unique(gsub(paste0('_',mod,'.*'), '',
                       grep(mod, colnames(dt), value=T)))
  ldply(races, function(race) {
    popcol = grep(paste0(race, '.*', mod, '.*_pop$'), colnames(p_attri), value=T)
    floodcol = grep(paste0(race, '.*', mod, '.*_popflood$'), colnames(p_attri), value=T)
    femacol = grep(paste0(race, '.*', mod, '.*_popFEMA$'), colnames(p_attri), value=T)
    varn <- c('per','floodper','FEMAper')
    statsdt <- p_attri[, c(sum(get(popcol), na.rm=T)/sum(get(paste0(ref, '_pop')), na.rm=T),
                           sum(get(floodcol), na.rm=T)/sum(get(paste0(ref, '_popflood')), na.rm=T),
                           sum(get(femacol), na.rm=T)/sum(get(paste0(ref, '_popfema')), na.rm=T)), by=byc]
    if (is.null(byc)) {
      statsdt <- data.table(bycol=rep('total', length(statsdt)), statsdt)
    }
    statsdt <- cbind(varn, statsdt, race, mod)
    paste0(race,'_',mod,'per')
    return(setDT(statsdt))
  })
}

#Build function for basic multiplot with weighted mean and quantile regression
multiplot <- function(dat, xvar, yvar, weightvar, facetvar) {
  p <- ggplot(dat, aes_string(x=xvar, y=yvar)) + 
    geom_point(alpha=1/4) + 
    geom_smooth(color='red', method='lm', aes_string(weight = weightvar)) +
    geom_quantile(quantiles=c(0.1, 0.25, 0.5, 0.75, 0.9), color='orange',
                  aes_string(mapping = weightvar)) +
    scale_y_log10() +
    geom_hline(yintercept=1, color='blue') +
    facet_wrap(as.formula(paste('~',facetvar)), scales = 'free_x') +
    theme_classic() +
    theme(axis.title =element_text(size=16),
          strip.text = element_text(size=16),
          strip.background = element_rect(colour="white"),
          text = element_text(size=16))
  print(p)
}

#Create new data for every combination of probabilities from Nstep
newprobmat <- function(step) {
  matlen <- (5*20/step+1)*(5*20/step+2)*(5*20/step+3)*(5*20/step+4)/24 #tweak on https://oeis.org/A151989 (20 coming from 100/5 loops)
  #Can also be written Reduce('*', (ncols*nvalues/step+seq_len(ncol-1)))/24
  
  predmat <- matrix(NA, matlen, 5)
  x=1
  for (i in seq(0L,100L, as.integer(step))) {
    for (j in seq(0L,100L-i, as.integer(step))) {
      for (k in seq(0L,100L-(i+j), as.integer(step))) {
        for (l in seq(0L,100L-(i+j+k), as.integer(step))) {
          predmat[x,] <- c(i,j,k,l,100-(i+j+k+l))
          x=x+1
        }
      }
    }
  }
  return(predmat)
}

#+----------------------------------- Import data ------------------------------------------------
#Get county codes
countycodes <- fread(file.path(datadir, 'geographic_codesWA_county.csv'))
county_notaxroll_GEOID <- setDT(countycodes)[COUNTY_NAME %in% c('Cowlitz', 'Lewis', 'Stevens', 'Jefferson', 'San Juan'),
                                             COUNTYFPL]
# #Read parcel-level data
<<<<<<< HEAD
p_attri <- st_read(dsn=gdb,layer="parcel_blocks_fill_flood_attri")
p_attri <- setDT(p_attri)
p_attri <- p_attri[FEMAFloodStatus == 1 & is.na(FEMAStudyStatus), FEMAFloodStatus := 0]
p_attri <- p_attri[is.na(FEMAStudyStatus), FEMAStudyStatus := 0]
save.image(file = "flood_vulnerability_model_dataimport.RData")

=======
# p_attri <- st_read(dsn=gdb,layer="parcel_blocks_fill_flood_attri")#takes a long time to run!
# p_attri <- setDT(p_attri)
# p_attri <- p_attri[FEMAFloodStatus == 1 & is.na(FEMAStudyStatus), FEMAFloodStatus := 0]
# p_attri <- p_attri[is.na(FEMAStudyStatus), FEMAStudyStatus := 0]
# 
>>>>>>> 740dd296cfa453ad08aa61dca1f8f1491c3b0396
# #Read census trast-level data
t_attri <- st_read(dsn=gdb, layer="tracts_floodfinal")
t_attri <- setDT(t_attri)
t_attri[, county := as.factor(substr(GEOID10_t, 1, 5))]

<<<<<<< HEAD
=======
save.image(file = "flood_vulnerability_model_dataimport.RData")
>>>>>>> 740dd296cfa453ad08aa61dca1f8f1491c3b0396
load(file = file.path(resdir, "flood_vulnerability_model_dataimport.RData"))
p_attri <- p_attri[t_attri[,c('GEOID10_t', 'DP0230001'), with=F], on='GEOID10_t'] #Add average household size
p_attri[, county := as.factor(substr(GEOID10_t, 1, 5))]

# #Export data for Jamie's presentation
# p_attrisub_pierce <- p_attri[county == '53053', 
#                           .(BuildNum, PolyID, HOUSINGADJ, PARCELPOP,
#                             white_BISG_pop, hisp_BISG_pop, black_BISG_pop, asi_BISG_pop, oth_BISG_pop)]
# write.csv(p_attrisub_pierce, 'parcel_attributes_pierce_forJamie20190122.csv') #In ArcGIS, make a quick right-join on parcel polygons

#Block group data
bg <- as.data.table(st_read(file.path(datadir, 'TIGER2017/2010_ACS_5YR_BG_53_WASHINGTON.gdb'), 
                            layer = 'ACS_10_5YR_BG_53_WASHINGTON')) %>%
  .[, county := as.factor(substr(GEOID10, 1, 5))] %>%
  .[!(county %in% county_notaxroll_GEOID),]

#+------------- block-group level analysis --------------------------------------------------------------
# ---- Format and visualize data ----
# poverty status B17021e1
# median household income B19013e1
# owner-occupied housing units median value (B25077e1)
# ratio of income to poverty level in past 12 months (C17002e1)
# contract rent/with cash rent (B25056e2)

#Compute percentage population flooded within each block group
wastats_bgsub <- bg[p_attri[, list(parcelfloodper = sum(floodpop)/sum(PARCELPOP)), by='GEOID10_bg'], on='GEOID10==GEOID10_bg']

#Compute average of state-standardized rent and house prise
wastats_bgsub[, `:=`(housing_standardized = B25077e1/mean(B25077e1, na.rm = T),
                   rent_standardized = B25056e2/mean(B25056e2, na.rm = T))][
                     , real_standardized := rowMeans(cbind(housing_standardized, rent_standardized), na.rm=T)]
#Compute average of county-standardized rent and house price
wastats_bgsub[, `:=`(B25077e1_countymean = mean(B25077e1, na.rm = T),
                     B25056e2_countymean = mean(B25056e2, na.rm = T)),
              by=county][
                , `:=`(housing_countystandardized = B25077e1/B25077e1_countymean,
                       rent_countystandardized = B25056e2/B25056e2_countymean),
              ][
                , real_countystandardized := rowMeans(cbind(housing_countystandardized, rent_countystandardized), na.rm=T)
              ]
  

#Format data for beta-inflated regression model
qplot(wastats_bgsub$parcelfloodper)#what is this: stat_bin
summary(wastats_bgsub$parcelfloodper)
which(wastats_bgsub$parcelfloodper < 0)
check1 <- wastats_bgsub[parcelfloodper > 1,]
wastats_bgsub[parcelfloodper > 1, parcelfloodper := 1] #Make sure that maximum flood % is 1 (some tiny decimals are messing things up)

#Get block-group percentage flooded population numbers
wastats_bg <- setDT(summod(p_attri, mod='BISG', 'last', byc='GEOID10_bg'))
wastats_bgcast <- dcast(wastats_bg, GEOID10_bg~varn+race, value.var = 'V1')%>%
  .[wastats_bgsub, on='GEOID10_bg==GEOID10']
BISG_colsbg <- c("per_asi", "per_black", "per_hisp", "per_white" , "per_oth")

#Round data based on smallest values
colSums(wastats_bgcast == 0, na.rm=T) #Check number of 0s in each column
wastats_bgcast[!is.na(per_asi), sapply(.SD, function(x) min(ifelse(x>0, x, NA), na.rm=T)), .SDcols = BISG_colsbg] #Check smallest value in each column
wastats_bgcast[, (BISG_colsbg) := round(.SD, 6), .SDcols = BISG_colsbg]

#Make sure that race-ethnicity probability vector only sums to 0-1 (sometimes tiny decimal mistakes due to rounding errors)
wastats_bgcast[, sum_BISG := Reduce('+', .SD), .SDcols = BISG_colsbg]
table(wastats_bgcast[, sum_BISG]) 
wastats_bgcast[sum_BISG > 0, per_oth := per_oth + 1 - sum_BISG] 

#Deal with 0s by replacing them all by 10-6 through multiplicative imputation 
wastats_bgcast[sum_BISG > 0, (BISG_colsbg) := lapply(.SD, function(x){x[x==0] <- 10^(-6); x}), .SDcols = BISG_colsbg]
wastats_bgcast[, sum_BISG := Reduce('+', .SD), .SDcols = BISG_colsbg]
BISG_colsbgclo <- paste0(BISG_colsbg,"_clo")
wastats_bgcast[sum_BISG > 0, (BISG_colsbgclo) := lapply(.SD, `/`, sum_BISG), .SDcols = BISG_colsbg]
wastats_bgcast[, sum_BISGclo := Reduce('+', .SD), .SDcols = BISG_colsbgclo]
table(wastats_bgcast[, sum_BISGclo]) #Make sure that add up to 1

#Create compositional data
BISGcomp_bg <- acomp(wastats_bgcast[!(sum_BISG %in% c(0, NA, NaN)), .SD, .SDcols = BISG_colsbgclo])

#Univariate histogram
colpal <- brewer.pal(n = length(BISG_colsbgclo), name="Set2")
uniBISG <- sapply(BISG_colsbgclo, function(cvar) {
  ggplotGrob(ggplot(wastats_bgcast, aes_string(x=cvar)) +
               geom_density(fill = colpal[which(BISG_colsbgclo == cvar)]) +
               scale_x_continuous(expand=c(0,0), name=cvar) +
               scale_y_sqrt() +
               theme_classic()
  )
})
do.call("grid.arrange", uniBISG)

#Make ternary density plots
ternmatrix_l <- sapply(seq_len(length(BISG_colsbgclo)-2), function(i) {
  sapply(seq(2, length(BISG_colsbgclo)-1), function(j) {
    xpc <- BISG_colsbgclo[i]
    ypc <- BISG_colsbgclo[j]
    wastats_bgcast[!(sum_BISG %in% c(0, NA, NaN)), tern_oth_BISG := Reduce(`+`, .SD), .SDcols = BISG_colsbgclo[-c(i, j)]]
    nr <- nrow(wastats_bgcast[!(sum_BISG %in% c(0, NA, NaN)),])
    samp <- wastats_bgcast[!(sum_BISG %in% c(0, NA, NaN)),]#[sample(nr, nr/3),]no sampling for bg
    ggplotGrob(
      ggtern(data=wastats_bgcast,aes_string(x=xpc,y=ypc,z='tern_oth_BISG')) +
        stat_density_tern(geom='polygon', aes(fill=..level..), bins=30, color=NA) +
        scale_fill_distiller(palette='Spectral') +
        theme(legend.position = 'none') +
        labs( x       = paste(gsub('per_', '', gsub('_clo', '', xpc)), '%'),
              xarrow  = paste(gsub('per_', '', gsub('_clo', '', xpc)), '%'),
              y       = paste(gsub('per_', '', gsub('_clo', '', ypc)), '%'),
              yarrow  = paste(gsub('per_', '', gsub('_clo', '', ypc)), '%'),
              z       = "Other",
              zarrow  = "Other") +
        theme_showarrows() +
        theme_clockwise()
    )
  })
})  
do.call("grid.arrange", list(grobs=ternmatrix_l[lower.tri(ternmatrix_l, diag=T)], 
                             ncol=nrow(ternmatrix_l))) #only keep unique combinations

# ---- Fit null model ----
nullmod <- gamlss(parcelfloodper ~ county,
                  nu.formula=~county, 
                  sigma.formula=~county,
                  family=BEINF,  #Very 0- and 1- inflated beta distribution
                  data= wastats_bgcast[!(is.na(get(BISG_colsbgclo[1])) | is.na(county)), 
                                       .(parcelfloodper, county)])
summary(nullmod)
AIC(nullmod)

# ---- Fit model % flooded parcels ~ real estate index and median household income ----
"See A8.2 p195 of gamlss manual of model description.
mu is the mean of the beta model, 
sigma is the shape,
nu is the probability ratio of p==0 and 
tau is the probability ratio of p==1"
check2 <- wastats_bgsub[is.na(parcelfloodper) | is.na(real_standardized),] #Only those that have no population
wastats_bgsub[!(is.na(parcelfloodper) | is.na(real_standardized)), 
              .N, by=county]

modat <- wastats_bgsub[!(is.na(parcelfloodper) | is.na(real_standardized) | is.na(B19013e1)), 
                       .(parcelfloodper, real_standardized, B19013e1, county),]
bg_real1 <- gamlss(parcelfloodper ~ real_standardized + B19013e1 + county,
                   nu.formula=~real_standardized + B19013e1 + county, 
                   sigma.formula=~real_standardized  + B19013e1 + county,
                   family=BEINF,  #Very 0- and 1- inflated beta distribution
                   data= modat)
summary(bg_real1, type='qr')
plot(bg_real1)

wastats_bgsub[!(is.na(parcelfloodper) | is.na(real_standardized)| is.na(B19013e1)), real1_fit := meanBEINF(bg_real1)] 
ggplot(wastats_bgsub, aes(x=real1_fit, y=parcelfloodper)) +
  geom_point() +
  geom_smooth(method='lm')


ggplot(wastats_bgsub[!(is.na(parcelfloodper) | is.na(real_standardized)| is.na(B19013e1)),]
       , aes(x=real_standardized, y=real1_fit, color=county)) +
  geom_point() +
  geom_smooth(method='gam')

pfunc <- getPEF(obj=bg_real1,term="B19013e1", plot=TRUE, type='response', how='median')
pfunc(10000)
getPEF(obj=bg_real1,term="real_standardized", plot=TRUE, type='response', how='median')

# ---- Fit model % flooded parcels ~ isometric log-ratio transformed race composition ----
ilrcols <- paste0('ilr', seq_len(ncol(BISGcomp_bg)-1)) #Vector of ilr column names
wastats_bgcast[!(sum_BISG %in% c(0, NA, NaN)), (ilrcols) := as.data.frame(ilr(BISGcomp_bg))] #Create ilrcols in wastats_bgcast
modat <- as.data.frame(wastats_bgcast[!(sum_BISG %in% c(0, NA, NaN) | is.na(county)),
                                      c('parcelfloodper', 'county', ilrcols), with=F]) #Subset columns for gamlss modelling (otherwise gives error message)

bg_ilr1 <- gamlss(parcelfloodper ~ ilr1 + ilr2 + ilr3 + ilr4 + county,
                  #sigma.formula = ~county,
                  nu.formula =~ ilr1 + ilr2 + ilr3 + county,
                  tau.formula =~ ilr1 + ilr2 + ilr3 + ilr4,
                  family=BEINF(),  #Very 0- and 1- inflated beta distribution
                  data= modat,
                  method=RS(),
                  control = gamlss.control(n.cyc = 20, c.crit = 0.001))

"Method vcov doesn't work so use 'qr'. Unrealiable for standard errors though, so exert caution... 
the default value vcov uses the vcov() method for gamlss to get the variance-covariance matrix of 
the estimated beta coefficients, see details below. The alternative qr is the original method used 
in gamlss to estimated the standard errors but it is not reliable since it do not take into the
account the inter-correlation between the distributional parameters mu, sigma, nu and tau."
summary(bg_ilr1, type='qr') #Sometimes just doesn't work
plot(bg_ilr1)

wastats_bgcast[!(sum_BISG %in% c(0, NA, NaN) | is.na(county)), ilr1_fit := meanBEINF(bg_ilr1)] 
ggplot(wastats_bgcast, aes(x=ilr1_fit, y=parcelfloodper)) +
  geom_point(aes(color=county)) + 
  coord_fixed() +
  #facet_wrap(~county, scales='free') + 
  geom_smooth(method='lm')

ggplot(wastats_bgcast, aes(x=per_white, y=parcelfloodper, color=county)) +
  geom_point() +
  facet_grid(~county) +
  geom_smooth(method='gam', span=1)

# ---- Create ternary diagram with new prediction matrix where color is % change from mean  # ----
subcols <- c('per_white_clo', 'per_black_clo', 'per_hisp_clo')
allcols <- c(subcols, BISG_colsbgclo[!(BISG_colsbgclo %in% subcols)])
predmat_bg <- rbind(as.data.frame(newprobmat(2)), 
                    c(rep(100/3, 3), 0,0))
colnames(predmat_bg) <- allcols
predmat_bgsub <- predmat_bg[predmat_bg$per_asi_clo == 0 & predmat_bg$per_oth == 0,]
#Add 10^06 to 0s and perform multiplicative reclosure
predmat_bgsub <- (predmat_bgsub/100+10^-3)/rowSums(predmat_bgsub/100+10^-3)
predmat_bgformat <- cbind(predmat_bgsub, as.data.frame(ilr(acomp(predmat_bgsub))))
colnames(predmat_bgformat) <- c(allcols, ilrcols)
predmat_bgformat$county <- as.factor(53045) #Take Yakima county as example
predmat_bgformat$ilr1_fit <- predict(bg_ilr1, newdata=predmat_bgformat[, c(ilrcols, 'county')]) #

#Check relationship between two counties
# predmat_bgformat$county <- as.factor(53041) #Take Yakima county as example
# predmat_bgformat$ilr2_fit <- predict(bg_ilr1, newdata=predmat_bgformat[, c(ilrcols, 'county')]) #
# ggplot(predmat_bgformat, aes(x=ilr1_fit, ilr2_fit)) + geom_point() +
#   geom_abline(slope=1, intercept=0)

#Reclose subset of three columns
setDT(predmat_bgformat)[, (subcols) := lapply(.SD, function(x) x/Reduce('+',.SD)), 
                        .SDcols = subcols]

#Check out predictions actually look
ggplot(predmat_bgformat, aes(x=per_white_clo, ilr1_fit)) + 
  geom_point()

#Find county with median intercept?

#Standardize predicted value by value when all columns have equal proportion
std <- predmat_bgformat[get(subcols[1])==get(subcols[2]) & 
                          get(subcols[1]) == get(subcols[3]), ilr1_fit]
predmat_bgformat[, ilr1_fitstd := (ilr1_fit-std)/std]
#predmat_bgformat[ilr1_fitstd < (-1), ilr1_fitstd := -1]

xpc <- subcols[1]
ypc <- subcols[2]
zpc <- subcols[3]
ggtern(data=predmat_bgformat,aes(get(xpc),get(ypc),get(zpc))) + 
  geom_point(aes(color=ilr1_fitstd), size=2) +
  # geom_interpolate_tern(aes(value=ilr1_fitstd, color=(..level..)),
  #                       size=1.3,
  #                       #colour   = "black",
  #                       formula  = value~x+y,
  #                       method = "lm",
  #                       binwidth = 0.05,
  #                       #buffer = 1.5,
  #                       n = 200) +
  #facet_wrap(~county) +
  scale_fill_distiller(palette='Spectral') + 
  scale_color_distiller(palette='Spectral') + 
  #theme(legend.position = 'none') +
  labs( x       = '', #paste(gsub('per_', '', gsub('_clo', '', xpc)), '%'),
        xarrow  = paste(gsub('per_', '', gsub('_clo', '', xpc)), '%'),
        y       = '', #paste(gsub('per_', '', gsub('_clo', '', ypc)), '%'),
        yarrow  = paste(gsub('per_', '', gsub('_clo', '', ypc)), '%'),
        z       = '', #paste(gsub('per_', '', gsub('_clo', '', zpc)), '%'),
        zarrow  = paste(gsub('per_', '', gsub('_clo', '', zpc)), '%')) +
  theme_showarrows() + 
  theme_clockwise()

# ---- Create ternary diagram with fitted predictions per county  ----
#Ternary diagram of predictions
terndat <- wastats_bgcast[!(sum_BISG %in% c(0, NA, NaN)) & county == '53077',]
ternmatrix_pred <- sapply(seq_len(length(BISG_colsbgclo)-2), function(i) {
  sapply(seq(2, length(BISG_colsbgclo)-1), function(j) {
    xpc <- BISG_colsbgclo[i]
    ypc <- BISG_colsbgclo[j]
    terndat[, tern_oth_BISG := Reduce(`+`, .SD), 
                            .SDcols = BISG_colsbgclo[!(BISG_colsbgclo %in% c(xpc, ypc))]]
    ggplotGrob(
      ggtern(data=terndat[get(xpc)>10^-3 & get(ypc)>10^-3 & tern_oth_BISG>10^-3,],
             aes_string(x=xpc,y=ypc,z='tern_oth_BISG')) + 
        geom_interpolate_tern(aes(value=ilr1_fit, color=(..level..)),
                              #colour   = "black",
                              formula  = value~poly(x,y, degree=1,raw=TRUE),
                              method = "lm",
                              binwidth = 0.005,
                              n = 200) +
        geom_point(aes(color=ilr1_fit), size=2, alpha=0.5) +
        #facet_wrap(~county) +
        #scale_fill_distiller(palette='Spectral') + 
        scale_color_distiller(palette='Spectral', name = '% population in flood zone') + 
        labs( x       = '', #paste(gsub('per_', '', gsub('_clo', '', xpc)), '%'),
              xarrow  = paste(gsub('per_', '', gsub('_clo', '', xpc)), '%'),
              y       = '', #paste(gsub('per_', '', gsub('_clo', '', ypc)), '%'),
              yarrow  = paste(gsub('per_', '', gsub('_clo', '', ypc)), '%'),
              z       = '', #"Other",
              zarrow  = "Other") +
        theme_showarrows() + 
        theme_clockwise() +
        theme(text = element_text(size=20)) +
        theme(legend.position = 'none')
    )
  })
})  

png(file.path(figdir, 'flood_vulnerability_model_bg_irl1mod_king.png'), width=20, height=12, units='in', res=400)
do.call("grid.arrange", list(grobs=ternmatrix_pred[lower.tri(ternmatrix_pred, diag=T)], 
                             ncol=nrow(ternmatrix_pred))) #only keep unique combinations
dev.off()

# ---- Create change matrix based on a set increase in race probability ratio ----
#ci,j in this case shows the increase in the response with a doubling of the ratio between the probability of i/probability of j
#e.g. the increase in the response if we go from 40% white/20% black to 80% white/20% black
invmu = ilrInv((bg_ilr1$mu.coefficients)[1:4], orig=BISGcomp_bg)

change_mat <- matrix(NA, 5, 5)
rownames(change_mat) <- names(b)
colnames(change_mat) <- names(b)
coeflen <- length(b)
rc <- 2 #change in ratio bi/bj (see p14, Chastin et al. 2015, S2 + Talarico MS thesis for a detailed equation) 
#(See table 4, chastin et al. for interpretation)
for (i in seq_len(coeflen)) {
  for (j in seq_len(coeflen)) {
    if (i==j) {
      change_mat[i,j] <- 0
    } else {
      change_mat[i,j] <- rc*(log2(invmu[i]/invmu[j])/4*coeflen^2)/2.78
    }
  }
}
change_mat

#+------------- tract-based regression analysis -------------
tBISG_cols <- c("white_BISG_per", "black_BISG_per", "hisp_BISG_per", "asi_BISG_per", "oth_BISG_per")
colnames(t_attri)
#Check distribution of percentage population within flood zone
t_attri[, .N, by='county']
ggplot(t_attri, aes(x=parcelfloodper)) + 
  geom_histogram() +
  facet_wrap(~county, scales='free_y')

#Make ternary racial composition distribution plots

#Make compositional data series

#Model


#Initial easy fixed effect model with average household size:
# --- % flooded population: zero-inflated zero regression
# --- Fixed effect (for now): average household size
# --- Random effect: county or watershed
ggplot(t_attri, aes(x=DP0230001, y=parcelfloodper)) + 
  geom_point() +
  geom_smooth() 

check <- t_attri[is.na(parcelfloodper) | is.na(DP0230001),] #Only those that have no population
housesize1 <- gamlss(parcelfloodper ~ re(random=~1|county), #DP0230001 + 
                     family=BEINF,  #Very 0- and 1- inflated beta distribution
                     data=t_attri[!(is.na(parcelfloodper) | is.na(DP0230001)) & DP0230001 > 0 &
                                    county %in% t_attri[,.N,by=county][N>10,]$county, 
                                  .(parcelfloodper, DP0230001, county),])
summary(housesize1)
plot(housesize1)
# Check fit values
t_attri[!(is.na(parcelfloodper) | is.na(DP0230001)) & DP0230001 > 0 &
          county %in% t_attri[,.N,by=county][N>10,]$county,][, housesize1_fit := meanBEINF(housesize1)] 
ggplot(t_attri, aes(x=housesize1_fit, y=parcelfloodper)) +
  geom_point() +
  geom_abline(slope=1, intercept=0)

ggplot(t_attri[!(is.na(parcelfloodper) | is.na(DP0230001)) & DP0230001 > 0,], aes(x=DP0230001, y=housesize1_fit)) +
  geom_point() +
  geom_abline(slope=1, intercept=0)


#Initial easy random effect model:
# --- % flooded population: zero-inflated zero regression
# --- Fixed effect (for now): average house price

#Initial more complex model:
# --- % flooded population: zero-inflated zero regression
# --- Fixed effect (for now): isometric log-ratio transformed race composition
# --- Random effect: county or watershed
#Potential implementations:
# --- GAMLSS (R package): generalized additive models for location, scale and shape
# --- JAGs (based on SEFS 590): mixture beta-bernoulli model with uninformative priors
# --- zoib (R package): bayesian approach


#+------------- Parcel-based regression analysis -------------
#https://stats.stackexchange.com/questions/259208/how-to-perform-isometric-log-ratio-transformation
#https://stats.stackexchange.com/questions/172905/bayesian-estimation-of-gee-models
# See van den Booqaart and Tolosana-Delgado (2013) - Analyzing Compositional Data with R
#     (sections 5.2 and 5.5 in particular for regression, advanced considerations and 7.1 for zeroes)
#---- Prepare data ----
BISG_cols <- c("white_BISG", "black_BISG", "hisp_BISG", "asi_BISG", "oth_BISG")
#p_regformat <- copy(p_attri) in final analysis if more RAM available
table(p_attri[last_pop>0, last_popflood/last_pop]) #Make sure that flood response is only 0-1
#Round data based on smallest values
p_attri[, sapply(.SD, function(x) min(ifelse(x>0, x, NA), na.rm=T)), .SDcols = BISG_cols]
p_attri[, (BISG_cols) := round(.SD, 6), .SDcols = BISG_cols]

#Make sure that race-ethnicity probability vector only sums to 0-1
#table(p_attri[, white_BISG+black_BISG+hisp_BISG+asi_BISG+oth_BISG]) 
p_attri[, sum_BISG := white_BISG + black_BISG + hisp_BISG + asi_BISG + oth_BISG]
p_attri[sum_BISG > 0, oth_BISG := oth_BISG + 1 - sum_BISG] %>%
  .[oth_BISG < 0, `:=`(white_BISG = white_BISG + oth_BISG,
                       oth_BISG = 0)] %>%
  .[white_BISG<0,`:=`(hisp_BISG = hisp_BISG + white_BISG,
                      white_BISG = 0)] %>%
  .[hisp_BISG<0,`:=`(black_BISG = black_BISG + hisp_BISG,
                     hisp_BISG = 0)]

#Deal with 0s by replacing them all by 10-6 through multiplicative imputation 
p_attri[sum_BISG > 0, (BISG_cols) := lapply(.SD, function(x){x[x==0] <- 10^(-6); x}), .SDcols = BISG_cols]
p_attri[, sum_BISG := white_BISG + black_BISG + hisp_BISG + asi_BISG + oth_BISG]
BISG_colsclo <- paste0(BISG_cols,"_clo")
p_attri[sum_BISG > 0, (BISG_colsclo) := lapply(.SD, `/`, sum_BISG), .SDcols = BISG_cols]
table(p_attri[, white_BISG_clo + black_BISG_clo + hisp_BISG_clo + asi_BISG_clo + oth_BISG_clo]) 

#Create compositional data
BISGcomp <- acomp(p_attri[!(sum_BISG %in% c(0, NA)), .SD, .SDcols = BISG_colsclo])

#Univariate histogram
colpal <- brewer.pal(n = length(BISG_colsclo), name="Set2")
uniBISG <- sapply(BISG_colsclo, function(cvar) {
  ggplotGrob(ggplot(p_attri, aes_string(x=cvar)) +
               geom_density(fill = colpal[which(BISG_colsclo == cvar)]) +
               scale_x_continuous(expand=c(0,0), name=cvar) +
               scale_y_sqrt() +
               theme_classic()
  )
})
do.call("grid.arrange", uniBISG)

#Make ternary density plots
ternmatrix_l <- sapply(seq_len(BISG_colsclo-1), function(i) {
  sapply(seq(2, BISG_colsclo), function(j) {
    if (i != j) {
      xpc <- BISG_colsclo[i]
      ypc <- BISG_colsclo[j]
      p_attri[!(sum_BISG %in% c(0, NA)), tern_oth_BISG := Reduce(`+`, .SD), .SDcols = BISG_colsclo[-c(i, j)]]
      nr <- nrow(p_attri[!(sum_BISG %in% c(0, NA)),]) 
      samp <- p_attri[!(sum_BISG %in% c(0, NA)),][sample(nr, nr/3),] 
      ggplotGrob(
        ggtern(data=samp,aes(get(xpc),get(ypc),tern_oth_BISG)) + 
          stat_density_tern(geom='polygon', aes(fill=..level..), bins=20, color=NA) + 
          scale_fill_distiller(palette='Spectral') + 
          theme(legend.position = 'none') +
          labs( x       = xpc,
                xarrow  = xpc,
                y       = ypc,
                yarrow  = ypc,
                z       = "Other",
                zarrow  = "Other") +
          theme_showarrows() + 
          theme_clockwise()
      )
    } 
  })
})  
do.call("grid.arrange", list(grobs=biplots_l[lower.tri(biplots_l, diag=T)], ncol=nPCs-1)) #only keep unique combinations


#Check whether % flooded is normally distributed by county and tracts

#---- Create generalized linear mixed-effect model with compositional data as independent variable ----
# See https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models/ for GLMM theory and interpretation
# and https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/ for logistic GLMM
subdt <- p_attri[county == '53077' & !(sum_BISG %in% c(0, NA)),]
BISGcomp_yakima <- acomp(subdt[, .SD, .SDcols = BISG_colsclo])

#Null model for AICc
pnull <- glmer(FloodStatus~ (1|GEOID10_t), data=subdt, family = binomial) 
summary(pnull)

#Mixed-effect model
p_glmemod <- glmer(FloodStatus~ilr(BISGcomp_yakima) + 1|GEOID10_t, 
              data=subdt, 
              family = binomial) 
summary(p_glmemod)
a = coef(p_glmemod)$GEOID10_t[,1] #p108
b = ilrInv(coef(p_glmemod)$GEOID10_t[1,-1],orig=BISGcomp_yakima)

#---- Create change matrix based on a set increase in race probability ratio ----
#ci,j in this case shows the increase in the response with a doubling of the ratio between the probability of i/probability of j
#e.g. the increase in the response if we go from 40% white/20% black to 80% white/20% black
change_mat <- matrix(NA, 5, 5)
rownames(change_mat) <- names(b)
colnames(change_mat) <- names(b)
coeflen <- length(b)
rc <- 2 #change in ratio bi/bj (see p14, Chastin et al. 2015, S2)
for (i in seq_len(coeflen)) {
  for (j in seq_len(coeflen)) {
    if (i==j) {
      change_mat[i,j] <- 0
    } else {
      change_mat[i,j] <- rc*(log2(b[i]/b[j])/4*coeflen^2)/2.78
    }
  }
}
#Exponentiating the change matrix gives us here specifically with a logistic regression the increase in odd ratios (very confusing)
change_mat_exp <- exp(change_mat) 

#---- Creating a matrix of new predictors spanning all values ----
predmat <- newprobmat(5)

#Add 10^06 to 0s and perform multiplicative reclosure
predmat_clo <- (predmat/100+10^-6)/rowSums(predmat/100+10^-6)

#Duplicate all probability combinations for each tract
ugeoid <- unique(subdt[, GEOID10_t])
tryl_geoid <- as.data.table(do.call(rbind, replicate(length(ugeoid), tryl, simplify=FALSE))) %>%
  cbind(rep(ugeoid, each=nrow(tryl)))
  
# calculate predicted probabilities and store in a list
acomp_pred <- acomp(predmat_clo[,1:5])
ilr(acomp_pred)
subdt[, trypred := predict(glmmod, type='response')]

ggplot(subdt, aes(x=white_BISG, y=trypred, color=GEOID10_t)) + 
  geom_point()

#blurg







